# 第2节 从一个实际案例入手

{% hint style="success" %}
**本节导读**

为了避免过于抽象的讲解，我们从本节开始引入一个具体的案例，并贯穿始终。读完本节，你将会了解：

* 为什么选择让AI写诗
* 一个写诗AI的输入输出是什么
* 如何将文字转变为适合机器阅读的数字
{% endhint %}

## 写诗AI

从本节开始，我们将会考察一个实际案例，一步步设计并完善大模型的方案。这个案例是我精挑细选出来的，个中原因我写在下面，算作本书来龙去脉的一点补充。

学习一门知识或技术，纯靠理论讲解往往效果不佳。理论总是过于抽象，许多人会迷失在复杂概念和繁琐的公式里，以至失去兴趣，无法掌握得透彻。我深知个中滋味，大模型的经典论文不可不读，但我当时读过就如囫囵吞枣，难以消化，以致始终未能领悟真谛。

直到2024年夏天。彼时，我刚从微软离职，打算休养生息，做一些有趣的事情。大模型方兴未艾，我一直想要弄清其底层的基本原理。虽然并未打算投身大模型的研发工作，但了解原理总没有坏处。

可是，网络上介绍大模型原理的内容并不多。人们更乐意讲解如何使用大模型开发奇思妙想的应用，这些当然是最有用的东西，但不符合我的胃口。虽然有一些人在努力向大家介绍大模型的底层原理，但就像当初看论文一样，一上来就是公式和注意力机制，看得头晕眼花。

功底深厚的人自然乐于阅读精炼学术范的文章，但更多像我一样的普通人还是希望通俗易懂、循序渐进，花最小的努力把一件事情学会。

当时，我观看了Andrej Karpathy的[Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)视频。他在视频中现场演示了如何从零编程实现一个GPT模型。在两个小时的时间内，他如同变魔法一样，赤手空拳，一步步从处理数据、构建模型、训练模型，最终得到了一个可以像莎士比亚一样说话的AI。

他的视频给了我很大启发，这正是我所期待的可以帮助更多人理解大模型原理的教程。不过，他的内容仍然有不小的门槛，我在看的时候需要反复拖动进度条，否则完全跟不上他的语速。从那时起，我开始有了本书的构想。我要为中国读者写一份更简单明了的教程。读者花的时间可以多些，但一定要容易理解，避免半途而废。

借鉴Andrej的做法，我也要提供一套可执行的代码，但其中的案例我希望更适合中国人。

事实上，之所以网上的资料只介绍大模型的理论基础，不提供实践环节，是因为我们不可能在自己电脑上训练大模型。一个GPT4级别的大模型，可能需要上万块GPU和大量电力消耗，总成本可能在上亿美金。想要自己在家训练一个大模型可谓是痴人说梦，就连开源的小尺寸大模型也不行。

为了解决这个问题，Andrej选择用一个小模型，让它学习以莎士比亚的语气说话。可以想象，小模型肯定不如大模型讲话那么流畅，但由于是莎士比亚风格，人们其实看不太出来语言效果的好坏，只觉得有趣。

这启发我选择实现一个写中文诗的AI。一方面，读者对诗的好坏应该不会过于敏感；另一方面，我可以方便地在这个小模型上讲解大模型训练的全流程，包括预训练、微调和与人类偏好对齐。每个训练阶段都属于主线任务写诗的一部分，读者可以很容易理解其中的道理。

虽然实际的大模型会更复杂，涉及更多技巧。但最底层的原理与本文案例别无二致，这些核心的部分，作为初学者应当首先掌握。

前情提要讲完，我们开始步入正题，看看写诗AI到底该怎么实现。

## 定义输入输出

在[第1节](di-1-jie-ru-he-rang-ai-xue-hui-shuo-hua.md)中曾经提到过，我们通过规定输入输出的映射来规定AI的行为。因此，如何收集数据，如何设计合理的输入输出就是第一件要考虑的事情。

在配套代码仓库中，我已经事先准备好了中国的所有古诗，放在[data.json](../data.json)文件里，以JSON格式保存。整个JSON对象是一个数组，数组中的每个元素是一首诗。我把开头第一首诗的部分贴在下面。

{% code overflow="wrap" %}
```json
[{"author": "太宗皇帝", "paragraphs": ["秦川雄帝宅，函谷壯皇居。", "綺殿千尋起，離宮百雉餘。", "連甍遙接漢，飛觀迥凌虛。", "雲日隱層闕，風煙出綺疎。"], "title": "帝京篇十首 一", "id": "3ad6d468-7ff1-4a7b-8b24-a27d70d00ed4"}, ...]
```
{% endcode %}

可以看到，每首诗包含作者、段落、标题、ID等字段。重新整理一下格式，就可以把这首诗写成我们日常所见的形式（本书后续的所有例子均不包含作者，因为生成的诗文不需要作者）：

> 帝京篇十首 一\
> 秦川雄帝宅，函谷壯皇居。\
> 綺殿千尋起，離宮百雉餘。\
> 連甍遙接漢，飛觀迥凌虛。\
> 雲日隱層闕，風煙出綺疎。

按照之前的计划，我们要把这类数据转换为合适的输入输出，输入是一段话，输出是这段话的下一个字。不过，真正开始实现，我们就会发现一个之前没有考虑到的问题。

输出是输入文本的下一个字，这很好理解。但输入文本到底取多长呢？如果默认把整首诗当作输入，只留最后一个字作为输出：

> **Input**:\
> 帝京篇十首 一\
> 秦川雄帝宅，函谷壯皇居。\
> 綺殿千尋起，離宮百雉餘。\
> 連甍遙接漢，飛觀迥凌虛。\
> 雲日隱層闕，風煙出綺疎\
> **Output**:\
> 。

是不是有点莫名其妙？输入一大堆，结果让模型预测最后的句号？这样的话模型肯定什么也学不会。

所以我们不能只让模型预测最后面的字，而是每个字它都要学会预测。但每个字所处的位置不同，我们只能用前面的文本预测后面的字，而不能用后面的文本预测前面的字，毕竟我们不可能倒着说话。

这就意味着，我们应该把一首诗拆分成多条数据。每条预测一个不同位置的字。比如，从这首诗拆分出的第一条数据为

> **Input**:\
> 帝\
> **Output**:\
> 京

没错，输入只有一个字。因为我们希望预测第二个字“京”，而能够用来预测“京”的就只有“帝”这个字了。以此类推，拆分出的第二条数据为

> **Input**:\
> 帝京\
> **Output**:\
> 篇

有没有找到规律呢？假设这首诗总共N个字，按照这种拆分方法，我们最后会得到N-1条数据。除了不预测第一个字，其它所有字都要预测。

模型由此便可以学会从任何位置开始往后接话，而不是局限于补齐某个特定位置的字。因为我们最终的目标是让模型能够独立生成一首完整的诗，它必须有能力从开头开始输出。当它输出一个字后，这个字可以加入输入文本，继续生成下一个字，直到整首诗全部生成。

不过，此时，我们又会面临另一个问题。按照这样的输入输出规则，模型岂不是必须从头阅读一首诗？在我们生成的N-1条数据中，输入全部都从这首诗的标题开头，然后是正文。如果我想要直接给它输入一个上句，让它输出下句，估计它又会傻眼了。

为了解决这个问题，我们或许可以把起始点变一变。刚才，我们每次都从第0个字开头，现在，我们可以从0 \~ N-1里面任选一个位置开头。不过，这样处理起来会有些麻烦，因为不光开头要变，结尾也要跟着变，可选的组合会非常多。

一个更好的方案是，把所有诗从头到尾连起来组成一个超级长的文本。然后，随机取一个位置作为起始点，将输入的最大长度固定为N。像刚才一样，分别取长度为1, 2, ..., N-1的文本作为输入，得到N-1条数据。这种方法生成的输入输出变化最多，起始位置、长度都不确定，模型因此可以学到更多样化的知识。

在我们的实现中，取N=256，随机选取起始位置后得到一个长度为256的文本，比如下面这段。

> 臨高城。\
> 嗟我久離別，羨君看弟兄。\
> 歸心更難道，回首一傷情。\
> \
> 金陵夜泊\
> 冷煙輕澹傍衰叢，此夕秦淮駐斷蓬。\
> 棲鴈遠驚沽酒火，亂鵶高避落帆風。\
> 地銷王氣波聲急，山帶秋陰樹影空。\
> 六代精靈人不見，思量應在月明中。\
> \
> 題天申宮三首 二\
> 中峰嘉樹綠陰陰，洞裏靈蹤已遍尋。\
> 欲下山門又迴首，數聲清磬白雲深。\
> \
> 賦得魚登龍門\
> 魚貫終何益，龍門在苦登。\
> 有成當作雨，無用恥爲鵬。\
> 激浪誠難泝，雄心亦自憑。\
> 風雲潛會合，鬐鬣忽騰凌。\
> 泥滓辭河濁，煙霄見海澄。\
> 迴瞻順流輩，誰敢望同升。\
> \
> 寄京城親友二首 一\
> 苦吟看墜葉，寥落共天

这里面包含了五首诗，其中第一首和最后一首不完整。按照前面的规则，从这256个字的文本可以进一步生成255条数据，每一条的输出对应于从第2个字开始的每个字。比如，第一条为

> **Input**:\
> 臨\
> **Output**:\
> 高

最后一条为

> **Input**:\
> 臨高城。\
> 嗟我久離別，羨君看弟兄。\
> 歸心更難道，回首一傷情。\
> \
> 金陵夜泊\
> 冷煙輕澹傍衰叢，此夕秦淮駐斷蓬。\
> 棲鴈遠驚沽酒火，亂鵶高避落帆風。\
> 地銷王氣波聲急，山帶秋陰樹影空。\
> 六代精靈人不見，思量應在月明中。\
> \
> 題天申宮三首 二\
> 中峰嘉樹綠陰陰，洞裏靈蹤已遍尋。\
> 欲下山門又迴首，數聲清磬白雲深。\
> \
> 賦得魚登龍門\
> 魚貫終何益，龍門在苦登。\
> 有成當作雨，無用恥爲鵬。\
> 激浪誠難泝，雄心亦自憑。\
> 風雲潛會合，鬐鬣忽騰凌。\
> 泥滓辭河濁，煙霄見海澄。\
> 迴瞻順流輩，誰敢望同升。\
> \
> 寄京城親友二首 一\
> 苦吟看墜葉，寥落共\
> **Output**:\
> 天

中间的数据以此类推。

唯一仍可能让人感到疑惑的是那些不完整的诗句。的确，由于设定的最大长度，我们截断了一些诗句。但这是一种合理的妥协。在后续小节中，我们会一步步了解到模型训练的三个阶段——预训练、微调和对齐。第一步预训练阶段，我们只需要让模型尽量多地掌握语言的底层逻辑，而不需要关心其输出的长度或格式。事实上，预训练模型的目的是让模型根据给定的输入输出下一个字，并且反复输出下去，直到规定的最大长度。此时的模型并没有完整的段落概念，它并不能用来对话（Chat），而只能用来补全（Completion）。因此，你会看到在我们这段文本中，一首诗结束了又接着下一首。我们希望模型能一首接一首地写诗，直到字数限制。

## 从文本到数字

如果你对计算机有所了解，你或许知道计算机只认得二进制数，其它的一概不懂。只有人能看得懂文字，任何信息在传入计算机前都要转变为数字。

最简单的转换是给每个字编一个序号，字和序号一一对应。这类似于一个字典，只要字典里面包含了所有可能出现的字，那么任何一句话就可以顺利地转译为一串数字。

在我们的案例中，并不需要一个包含所有汉字的字典。为了节约存储空间，我们可以用一个更小的字典，里面只包含训练数据中出现过的字。毕竟，历史上的所有古诗基本上涵盖了大部分写诗所需的字，用这些就够了。经过统计，我们的训练数据中共出现了8548个不同的字，把他们按照出现的频率列出来大概长这样：

> 不，人，山，一，日，無，風，中，上，有，...(此处省略8528个字)，寗，犗，媍，璡，绵，靧，輣，儚，躒，銚

最热门的10个字大家都认识，但最冷门的10个字恐怕少有人能念得出来。

于是，我们可以直接根据这个字典，按照顺序给每个字赋予唯一的编号，从0直到8547。字的排序其实并不重要，无论是按频率排序还是随机排都没有关系，因为这一步只是给它们一个代号，并不会影响它们在后续环节的发挥。在我们的代码中，字是随机排序的。

把这个字典应用到上节的数据上，我们就拥有了一个用数字表示的输入输出的集合。举个例子，假设其中有两条数据长这样：

> **Input1**: 白日依山盡，黃河入海
>
> **Output1**: 流
>
> **Input2**: 無端五十絃，一絃一柱思華年。
>
> 莊生曉夢迷蝴蝶，望帝春心
>
> **Output2**: 託

按照字典编码后就变成了：

> **Input1**: \[4403, 2704, 345, 1642, 4450, 8347, 8252, 3407, 536, 3503]
>
> **Output1**: 3486
>
> **Input2**: \[3875, 4861, 234, 749, 5129, 8347, 178, 5129, 178, 2952, 2048, 5823, 1846, 20, 2, 5782, 4276, 2803, 1307, 6978, 6170, 6171, 8347, 2844, 1807, 2732, 2009]
>
> **Output2**: 6436

值得注意的是，标点符号和换行也要考虑在内，因为模型需要学习如何使用它们。你可以试着将每个字符与编码后的数字一一对应起来。你会发现，8347代表逗号，20代表句号，2代表换行。

现在，训练数据终于就绪了。是时候有请我们的大杀器——神经网络闪亮登场，看它如何从数据中学会写诗。
