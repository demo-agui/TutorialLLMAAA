# 第9节 AI也要讲基本法

{% hint style="success" %}
**本节导读**

智能是一把双刃剑，越高的智能，越容易脱离人们的约束。在真正开始使用AI之前，应当确保它符合人类社会的法律、道德观和价值观。做一个有益的工具，而不是潜在的威胁。读完本文，你将会了解

* AI的不安全因素
* 如何与人类偏好对齐
* 如何让AI写的诗符合我的喜好
* DPO vs RLHF：不同的对齐技术
{% endhint %}

## 潜在风险

虽然模型已经可以对话，能够答复用户的问题，但在实际体验中很可能并不好用。

回想与人聊天的情况，和一些人对话能感到如沐春风，和另一些人则话不投机半句多。表达同样的意思，有的人可以说的很让人信服，而有的人则只会让人感到冒犯。这是聊天的艺术，不光人类，AI也需要考虑这个问题。

作为人类，我们其实并不需要高超的聊天技能，因为每个人从事不同的职业，社交不一定是必需品。但作为一个面向全世界用户的聊天AI，它需要考虑的就多了。它必须具有中立的普世价值观，不能歧视少数群体，不能表达极端观点。要遵守各个国家的法律，尊重不同民族的习俗。说话态度要积极向上，多传递正能量，弘扬社会正义。面对用户不合理的请求，一定要拒绝，但也不能激怒对方。总之，面对众口难调的用户，AI必须谨小慎微，不能说错话。

如果ChatGPT是一个未与人类偏好对齐的AI，别有用心的人就可能引诱AI透露危险的信息。比如，人们可以问AI“如何在家制作炸药？”这不是个玩笑。AI上知天文下知地理，制作炸药的步骤它可以信手拈来。以前，一个反社会分子想要知道怎么制作炸药，必须得学几年化学。然而现在，AI可以手把手教他。看起来人畜无害的AI此刻就成了恐怖分子的帮凶。类似的场景数不胜数，聪明人既可以做出科学突破，也可能摇身一变成为邪恶的源泉。因此，与人类偏好对齐非常重要，只有这样，AI才能真正造福人类，成为值得信赖的伙伴。

## 如何对齐？

那么该如何做呢？回想前面的预训练和指令微调，模型其实都是在学习现成的句子。模型看到什么文字，就会模仿那样说话。在海量的训练数据中，一定会掺杂着不安全、不道德的内容。一个简单直接的办法是从训练数据中去掉它们，眼不见为净。如果模型从来没看过关于制造炸药的知识，它自然也就没法告诉用户。

但这种直觉上正确的做法其实并不可行。一方面，就算模型没读过如何制造炸药，它也完全可以通过化学知识和关于炸药的常识自己推断出来。另一方面，很多语料是没法清晰定义安全性的。一个生物化学方面的论文，既是社会发展的推动力，也可能蕴含着危险的因素，我们不可能因为研究有可能产生危害就不做这个研究。很多事情都是双刃剑，如果拒绝一个语料可能带来的坏处，就同时隔绝了它所能带来的好处。此外，与人类偏好对齐并不仅仅需要考虑安全性问题，价值观、道德、礼貌等方面也同样重要。难道要把不礼貌的语料从训练数据中全部去掉？那样的话训练数据估计就没多少了。

所以，我们不可能在现有的训练模式中解决这个问题，必须用一种新的方法。

如果仔细思考训练数据的模式，可以发现，我们一直在教模型应该怎样做，却从来没告诉过它不该怎样做。而与人类偏好对齐最合理的方式应该是同时告诉它不要怎样做。也就是说，我们应该制定一些规则，然后生成一大批新的训练样本。在这些样本中，一半作为正样本，告诉模型你可以学着这样说话。另一半作为负样本，告诉模型你不要说这样的话。模型在训练过程中同时观看正负样本，从而能够明辨是非，形成自己的道德观和价值观。

既然如此，让我们想想如何具体实施。在[第7节](di-7-jie-cong-zhi-zhang-dao-tian-cai.md)的图12中，训练的目标是让最后实际输出的概率分布与期望ID一致，训练过程中会不断优化使得误差越来越小。想象如果我们现在有两个训练样本，一正一负。正样本其实和以往的训练数据用法一样，只要优化它与期望输出之间的差距就好。但负样本呢？如果把负样本放进去优化，模型就会越学越差。所以我们应该反过来，让模型学习增大负样本实际输出与期望输出之间的差距。这个差距越大，说明模型越来越不会像负样本那样说话。

于是，我们可以重新设计优化目标。把原本的交叉熵损失作为正损失，然后额外增加一个负样本的交叉熵损失，作为负损失。我们不是把两个损失加起来，而是用正损失减去负损失作为最终的损失。此处的减法相当于优化负的负样本损失。可以这样理解，由于优化交叉熵损失就是减小实际输出和期望输出之间的差距，那么优化负的交叉熵损失显然就是增大差距了。下式概括了优化目标的形式，通过调整模型参数让正样本损失越来越接近0、负样本损失越来越大，两者之差越来越小。其中，θ代表模型的参数，p代表正样本，n代表负样本。

$$
\min_{\theta} \left( \text{loss} \left( \theta, p \right) - \text{loss} \left( \theta, n \right) \right)
$$

优化新的损失可以让模型既接近正样本又远离负样本，一举两得。可能有读者会感到疑惑，为什么不单独优化负样本呢，只保留带负号的第二项不就行了？没错，如果单独优化负样本，模型的确会远离负样本，但这种情况下很容易导致模型本身的语言能力退化，以至于连话都不会说了。道理很简单，我们的负样本损失只要求模型的输出远离预期，然而这个要求简直太容易达到了，胡言乱语甚至输出随机字符都可以满足这个要求。但我们想要的其实是在正常说话的同时远离负样本，这就意味着必须有一个正样本时刻牵扯着负样本，强迫模型优化正负样本之间的差距。

这种训练方式看起来不难，但实际操作上有一些麻烦。最大的问题是正负样本损失不具有可比性。比如，还是上节的咨询案例，给它添加一个负样本。让负样本的回答缺乏咨询师应有的同理心，冷漠、粗鲁、随意地应付来访者。

> 用户问题：我最近很郁闷，能告诉我该怎么办吗？
>
> 正样本回答：我理解你现在的感受，感觉郁闷常常会让人有些迷茫。你能告诉我是什么让你感到郁闷吗？
>
> 负样本回答：你先说说你为什么郁闷吧。

按照设定，我们希望训练模型，让正样本的损失变小，同时负样本损失变大。然而，如果我们在训练之前就计算正负样本的损失，会发现它们本身就有一定差距。而这个差距是否意味着模型本身就对正负样本有所偏好呢？

其实不是。要知道，用于训练的数据集包含了成千上万的样本，里面任何一句话，单独拿出来计算误差，结果都是不同的。也就是说，我们其实是在努力让整个数据集总体误差更小，而不是盯着某条数据优化。这样的话，单独的一条正样本和负样本误差不同很正常，而且优化的目标也不是缩小或增大这个差距。我们需要一个更明确，且与特定数据无关的衡量标准。

回顾最初的目的，我们希望对齐后的模型相对于原始模型更能区分正样本和负样本。于是，这里其实有一个天然的标准，就是原模型。虽然正负样本的误差不可比较，但正样本在原模型和新模型上的误差则可以比较，而且非常契合我们的需求。这就好像自我成长，模型和过去的自己比较，逐渐成长为期待的样子。

于是，我们把模型复制一份。一个作为原模型，也叫做参考模型，另一个是新模型，将成为对齐后的模型。然后，我们锁定参考模型的参数，让它保持不变，构造下式所示的损失函数。其中，带下标0的θ表示固定的参考模型。

$$
\min_{\theta} \left( \text{loss} (\left( \theta, p \right) - \text{loss} \left( \theta_0, p \right) ) + (\text{loss} \left( \theta_0, n \right) - \text{loss} \left( \theta, n \right)) \right)
$$

在这个新的损失函数中，优化目标变成了两项。第一项是新旧模型在正样本上损失之差，这个差越接近负无穷，意味着新模型的正样本误差越小。第二项是旧新模型在负样本上损失之差，这个差越接近负无穷，意味着新模型的负样本误差越大。按照这个目标优化，新模型就会越来越擅长预测正样本，同时更不善于输出负样本。

现在，回到我们的主线任务，尝试用这种方法调教我们的写诗模型。

## 偏好五言诗

首先，我们要确定想要对齐的偏好是什么。在写诗的过程中，诗人可以有很多个人考量的角度，比如格律（七言还是五言，律诗还是绝句）、流派（豪放还是婉约）、内容（叙事还是抒情）等等。让模型偏好某种风格，正如古代的诗人一样。这种偏好不代表水平高低，正如将模型与人类偏好对齐也并非模型能力上的进步。出于教学的目的，我选择让模型偏好五言诗。这种风格的改变最容易被读者观察到，而且也方便我们构建训练数据。

接下来，我们把训练数据中的五言诗和非五言诗分成两组。将其随机配对，形成正负样本的集合。每一对正负样本大概长这样。

> 正样本：\
> \<INS>請用以下題目寫一首詩\<INP>宿壽安甘棠館 二\<RES>山空蕙氣香，乳管折雲房。\n願值壺中客，親傳肘後方。\n三更禮星斗，寸匕服丹霜。\n默坐樹陰下，仙經橫石床。
>
> 负样本：\
> \<INS>請用以下題目寫一首詩\<INP>遣懷\<RES>落魄江南載酒行，楚腰腸斷掌中輕。\n十年一覺揚州夢，贏得青樓薄倖名。

可以看到，无论正样本还是负样本，它们的格式与指令微调时并无两样，因为我们并不想改变模型与人们交互的方式。正样本是随机取的一首五言诗，来自唐代诗人尚颜的《宿寿安甘棠 二》。负样本则是随机取的一首七言诗，正是著名诗人杜牧的《遣怀》，想必许多读者有所耳闻。

将这对样本分别输入新旧两个模型，可以得到四个损失值，再将其带入上面的总损失公式即可。训练一段时间后，就能看到模型变得更倾向于生成五言诗。

```
<Omit many iterations here...>

Epoch 2, step 0, train loss 0.0000, evaluate loss 0.6818, train reward margin 0.0000, evaluate reward margin 0.2581
Generate a complete poem for title 春夜喜雨:
Aligned model:
翠紗復掩涴，色轉目猶輕。
素巷風無事，清池月不寒。
雲收爐尚望，日臥菊初明。
翠貼霏逾老，香藏遠獨紅。
Reference model:
黔城風光雪老遷，不將岸上林間樹。
莫道鸚鵡詩想熟，此催黃柳老風摧。
```

## DPO与RLHF

在本节的最后，让我们把视角稍微拉回现实世界。事实上，本节介绍的方法称为DPO（Direct Preference Optimization，直接偏好优化），它只是对齐方法的一种，甚至不是最主流的。

OpenAI最初提出的方法要更有名气一些，称为RLHF（Reinforcement Learning from Human Feedback，人类反馈的强化学习）。当时，为了将模型的价值观与人类对齐，研究者们认为是时候引入人工反馈了。但如果只是单纯人工标注数据然后丢给模型学习，投入产出比太低，毕竟OpenAI的研究人员没那些闲工夫天天标注数据。于是，他们开始琢磨强化学习这条路。

强化学习，简单来说就是一种终极目标可衡量的学习过程。比如下围棋，下到最后输或是赢结果非常明确，绝不存在争议。几乎所有的游戏、运动类项目都可以认为是强化学习任务，这也解释了为什么基于强化学习的AlphaGo系列最先在围棋和Dota等游戏中获得成功。为什么能衡量终极目标这一点如此重要呢？想象我们人类自己，有明确目标的情况下，做事往往更有动力。其中的道理并不复杂，抵达终极目标的道路是有限的，从固定的目标倒退过程，我们就有可能找到抵达目标的最短路径。至少，最短路径理论上存在。反之，如果没有目标，我们甚至无从思考努力的方向。这个世界的可能性无限大，缺乏目标约束的人生可能会走得非常随机，许多人会在中途失去动力。

然而，说话并不是一个有明确目标的任务，所以历史上并没有人用强化学习打造语言模型。但回到偏好对齐这个任务上，它的目标似乎稍微明确了一些。假设我是一个价值观评判师，符合我价值观的文本打1分，不符合的打评-1分。OpenAI的人认为我的价值观非常符合他们对ChatGPT的期待，于是邀请我去给ChatGPT打分。他们使用一种复杂的强化学习框架（PPO），每次让ChatGPT生成一个回答，交给我打分。打分后，强化学习框架会根据这个分数微调模型参数，以求下次生成的回答得分更高。经过这套训练流程，模型会越来越符合我的评价，从而实现与我对齐。当然，以上是一个通俗解释。实际上，我的作用被一个奖励模型（Reward Model）取代，它可以更快地为每个回答打分。毕竟，真人在这个训练流程中显然跟不上节奏。

不过，RLHF是一个极度复杂的框架，需要额外的奖励模型，强化学习训练也容易不稳定。DPO的出现解决了这些问题。这个来自于斯坦福大学的研究团队，从理论上证明了使用正负样本对的训练本质上和强化学习一样，而形式上却简单得多。本书为了演示效果和方便理解，选择了使用DPO这种训练方式。

必须承认的是，RLHF从提出至今仍然占据着主流地位，几个头部商用大模型（ChatGPT，Claude，Gemini）都采用了这种方法。虽然DPO在理论上很完美，却始终没能代替RLHF，只是在较小规模的模型上有所应用，在超大模型中表现欠佳。有许多学者尝试改进DPO，并提出了许多理论以说明DPO相比于RLHF的弱点。这里面的门道很深，本文就此打住。

至此，我们用9节内容介绍了如何训练一个能按照个人喜好写诗的AI，并讲解了数据处理、模型设计、训练方法的完整流程。读到这里的读者，恭喜你们应该已经了解了大模型的基本工作原理。

最难的环节已经度过，在最后两节，我会给学有余力的读者介绍配套代码的实现细节，以及如何拥抱AI的新世界。
